
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Sparsely gated mixture of experts - paper notes - Vivek Narayanan's blog</title>
  <meta name="author" content="Vivek Narayanan">

  
  <meta name="description" content="“The sparsely gated mixture of experts layer” is an interesting paper by Geoffrey Hinton and others at Google. When you have a large number of &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://vivekn.com/blog/2017/02/05/sparsely-gated-mixture-of-experts-paper-notes">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Vivek Narayanan's blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'], ["\(", "\)"] ],
        displayMath: [ ['$$', '$$'], ["\[", "\]"] ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
      //,
      //displayAlign: "left",
      //displayIndent: "2em"
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-44840523-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Vivek Narayanan's blog</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:vivekn.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Sparsely Gated Mixture of Experts - Paper Notes</h1>
    
    
      <p class="meta">
        








  


<time datetime="2017-02-05T21:07:00-08:00" pubdate data-updated="true">Feb 5<span>th</span>, 2017</time>
        
           | <a href="#disqus_thread"
             data-disqus-identifier="http://vivekn.com">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>“<a href="https://openreview.net/pdf?id=B1ckMDqlg">The sparsely gated mixture of experts layer</a>” is an interesting paper by Geoffrey Hinton and others at Google. </p>

<p>When you have a large number of training examples and you want to extract the most out of your data, one way of doing it would be increasing the capacity of the machine learning models. But this also means that for small incremental gains in accuracy, you have to increase the number of parameters by a few orders of magnitude. This is computationally very expensive, if the entire network is activated for each example. But only certain parts of the network might be needed for different types of inputs. This could be done deterministically by training multiple models and selecting a particular model based on certain heuristics.</p>

<p>But it might be hard to come up with such heuristics and decide what to include in each model. This paper introduces the concept of a trainable gating layer. The model is a collection of experts, each individual expert can be a feedforward neural network, the gating layer selects a small number of experts for each input.</p>

<script type="math/tex; mode=display"> y = \sum_{i=1}^{n} G(x)_i E_i(x) </script>

<script type="math/tex; mode=display"> G(x) = Softmax(PreserveTopK(W_gx)) </script>

<p>The gating function is a modified softmax layer, which sets the values not in the top <script type="math/tex">k</script> to <script type="math/tex">-\infty</script>. It can be trained jointly with the expert networks using gradient descent. This sparsity ensures that both training and prediction can be really fast as the zeroed out networks don’t need to be evaluated or backpropagated through. The final prediction is a sparse linear combination of the experts. </p>

<p>There are some issues with this approach though, during training the weights on some of the experts can get larger and due to reinforcement only these experts would be trained. The authors suggest two ways to solve this.</p>

<ol>
  <li>
    <p>Adding Gaussian noise to the outputs before applying the softmax.</p>
  </li>
  <li>
    <p>Introduce a loss penalty for unbalanced distribution of examples to experts, penalizing high variance in the distribution of examples.</p>
  </li>
</ol>

<script type="math/tex; mode=display"> \text{Importance}_i = \sum_{x \in X} G_i(x)  </script>

<script type="math/tex; mode=display"> \text{loss}_{\text{imp}} = w_i CV(\text{Importance}_i) </script>

<p>The coefficient of variation (<script type="math/tex">CV</script>) is the ratio of standard deviation to the mean of a random variable. </p>

<p>If there are hundreds of experts and mini-batch training is used, each expert network will only receive a small number of examples per batch, which would be computationally inefficient. So, the batch size should be as large as possible. But for a large batch size, all the activations and weight updates should be stored in memory, which is also infeasible for a large number of neural networks. They suggest using a very large batch size for the gating network and then sharding the individual models across multiple machines and training with model parallelism. This way individual networks will still have reasonable batch sizes and it also helps with memory usage during evaluation time.</p>

<p>This is an interesting way of increasing the capacity of the model while keeping computational costs reasonable. Different experts can act on different properties of the data and all of them need not be active simultaneously.</p>

</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Vivek Narayanan</span></span>

      








  


<time datetime="2017-02-05T21:07:00-08:00" pubdate data-updated="true">Feb 5<span>th</span>, 2017</time>
      


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://vivekn.com/blog/2017/02/05/sparsely-gated-mixture-of-experts-paper-notes/" data-via="_vivek_n" data-counturl="http://vivekn.com/blog/2017/02/05/sparsely-gated-mixture-of-experts-paper-notes/" >Tweet</a>
  
  
  
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2016/08/07/using-reinforcement-learning-to-train-neural-networks/" title="Previous Post: Using reinforcement learning to train neural networks.">&laquo; Using reinforcement learning to train neural networks.</a>
      
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      
      <li class="post">
        <a href="/blog/2017/02/05/sparsely-gated-mixture-of-experts-paper-notes/">Sparsely Gated Mixture of Experts - Paper Notes</a>
      </li>
      
    
      
      <li class="post">
        <a href="/blog/2016/08/07/using-reinforcement-learning-to-train-neural-networks/">Using Reinforcement Learning to Train Neural Networks.</a>
      </li>
      
    
  </ul>
</section>




  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - Vivek Narayanan -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'viveknblog';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://vivekn.com/blog/2017/02/05/sparsely-gated-mixture-of-experts-paper-notes/';
        var disqus_url = 'http://vivekn.com/blog/2017/02/05/sparsely-gated-mixture-of-experts-paper-notes/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
