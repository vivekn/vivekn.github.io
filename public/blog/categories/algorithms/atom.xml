<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: algorithms | Vivek Narayanan's blog]]></title>
  <link href="http://vivekn.com/blog/categories/algorithms/atom.xml" rel="self"/>
  <link href="http://vivekn.com/"/>
  <updated>2016-08-07T19:20:05-07:00</updated>
  <id>http://vivekn.com/</id>
  <author>
    <name><![CDATA[Vivek Narayanan]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The Travelling Santa Problem]]></title>
    <link href="http://vivekn.com/blog/2012/12/30/travelling-santa-problem/"/>
    <updated>2012-12-30T17:48:00-08:00</updated>
    <id>http://vivekn.com/blog/2012/12/30/travelling-santa-problem</id>
    <content type="html"><![CDATA[<p>I spent the past couple of days on a problem called the “Travelling Santa Problem”, a variant of the NP complete “<a href="en.wikipedia.org/wiki/Travelling_salesman_problem">Travelling Salesman Problem</a>”. There is an ongoing competition on Kaggle, a site which hosts various machine learning and optimization contests. In the original TSP problem you need to find a tour of a graph, visiting every vertex exactly once, minimizing the total cost. Since there can be an arbitrarily large (exponential in number of vertices) number of such paths (Hamiltonian cycles), no polynomial time exact algorithm is known for this problem. Applications of TSP are in the areas of scheduling, logistics and implementation of electronic circuitry. Kaggle’s <a href="http://www.kaggle.com/c/traveling-santa-problem">version</a> of the problem gives you a set of 150000 points on a two dimensional plane, and you need to find two paths, that visit all the points and have no edges that are common to both the paths, ie, disjoint paths. The aim of the contest is to find the minimum distance and your score is the distance on the longer path.</p>

<p>If you visualize the set of points, you get a picture of Santa.</p>

<p><img alt="image" height="474" src="https://kaggle2.blob.core.windows.net/competitions/kaggle/3294/media/santa.png" width="473" /></p>

<p>They provided a benchmark of random paths to get started with, that is two completely random sequences of points. It had a score of about 1.3 billion, and at that time the leader had a score of 6.5 million. I decided to start with a very simple heuristic, which was to sort the points according to their x coordinate and connect the path from left to right. With this I could find a single path, but what about the second path? The disjoint path condition makes this a little trickier. But given the first path, it should be easy to check if adding a node to the second path will be valid. Since every node would have a maximum of two connections, we just need to store the ids of the previous and next nodes for each node in the first sequence. So using this, I sorted the points according to the y coordinate and started building the second path from top to bottom, if there was an edge that would be repeated from the first path, I would simply select a random vertex and make a connection. So now that I had two disjoint paths, I decided to make a submission, the score turned out to be 400 million. Much better than random, but way behind the leaders and quite inefficient. If you come to think of it, this approach would create a very zig-zag path on random data. The disjointness condition forces you to add random edges to the second path, which take the distance up even further. So time for a different strategy.</p>

<p>So the next heuristic I tried was the nearest neighbour approach. If you connect each node to its nearest neighbour, you should get a shorter path and this is quite intuitive. But the problem here is, it would have a complexity of O(n^2) and there are 150,000 points, go figure. A <a href="en.wikipedia.org/wiki/K-d_tree">k-d tree</a> can find the nearest neighbour in O(n lg n) time, but its implementation is non trivial and it would be pretty much useless for generating a second disjoint path, we would still have to rely on random links. Time for an approximation of nearest neighbours: its highly likely that the nearest neighbour would lie on one of the points closest by x or y coordinates. This can be found by simply sorting the points and considering the <strong>k</strong> nearest such points and finding the nearest neighbour (we can also check for the disjointness condition, while doing so). Doing this on the nearest 100 points for each node, got me a solution with a score of 25 million. I decided to increase the number of points, and at around 4000 points, I could get a solution of around 8.5 million and the code runs in a minute. Within 30% of the leaders, not bad, huh?. So now, how can this be improved further.</p>

<p>I opened CLRS to look for ideas, there was an approximation scheme called the 2-approximation scheme which is based on finding vertices on a walk of a minimum spanning tree. CLRS has an elegant proof of how it is less than twice the minimum possible value, and in practice much better than that. But the problem is in creating the minimum spanning tree. Using Prim’s algorithm would have a complexity of O(n^2 lg n), which is not feasible. It turns out that an MST for a Euclidean plane can be created in O(n lg n) time using a <a href="en.wikipedia.org/wiki/Voronoi_diagram">Voronoi diagram</a> or a <a href="en.wikipedia.org/wiki/Delaunay_triangulation">Delaunuay triangulation</a>. As I had no background in computational geometry, I decided not to go down that path. Creating an approximate MST taking 40 nearest by x or y coordinates yielded a poor result.</p>

<p>An optimization technique, that was mentioned a lot in the forums was <a href="http://en.wikipedia.org/wiki/2-opt">2-opt</a>. It is a local optimization technique, that involves taking two adjacent edges, and replacing them with a different combination that reduces the distance. If you have a sequence of points ABCD, you replace it with ACBD if it has a lower cost. I tried this technique, by randomly picking consecutive edges and swapping vertices if they had a lower cost. Even after 1 billion iterations and 45 minutes, it only yielded an improvement of 0.1%. It was getting stuck at a local minimum. Then I ran the optimization in a sequential order of all vertices in the path, instead of random choice, it converged in 5-6 iterations and yielded a 2-3% improvement.</p>

<p>There is a randomized algorithm known as <a href="en.wikipedia.org/wiki/Simulated_annealing">simulated annealing</a> based on the metallurgical principle of annealing, a metal is heated to a high temperature and allowed to cool slowly to remove crystal imperfections. Heating allows a different configuration to be reached, and on cooling the imperfection is removed. Here, perturbations of the sequence are created using 2-opt and during the high temperature phase, changes are accepted even if it increases the total cost. In the cooling phase the acceptance rule is tightened until a point is reached where only those moves that reduce the cost are accepted. This helps in finding a global optimum. Unfortunately, this algorithm didn’t improve the solution by much even after running for an hour.</p>

<p>I could get one path at around 7.1 million and the second at 8.25 million under 5 minutes of running time using sequential 2-opt. So the difference in the two paths could be the main culprit. The <a href="http://www.research.att.com/~dsj/papers/TSPchapter.pdf">Lin - Kernighan</a> heuristic is considered to be one of the best heuristics for TSP and it is a generalization of 2-opt. But my motivation to read more about it diminished after seeing some posts in the competition forum, that some of the leaders ran their programs for days, and worse, even weeks to get their results. In my opinion running a program for days to achieve a very marginal improvement is totally not justified. This was really disturbing and I decided to quit the competition.</p>

<p>What I learned from this competition was that NP complete problems could be approximated reasonably within a short amount of time, and the last mile takes a <strong>humongous</strong> effort, and whether its worth it is questionable. I also got to know about some randomized algorithms and Monte Carlo simulations. You can find my C++ code over <a href="https://github.com/vivekn/tsp-heuristics">here</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Roll your own autocomplete solution using Tries]]></title>
    <link href="http://vivekn.com/blog/2012/02/25/roll-your-own-autocomplete-solution-using-tries/"/>
    <updated>2012-02-25T17:48:00-08:00</updated>
    <id>http://vivekn.com/blog/2012/02/25/roll-your-own-autocomplete-solution-using-tries</id>
    <content type="html"><![CDATA[<p>You might have come across many websites with autocomplete suggestions, most notably Google.</p>

<p><img src="http://storage.googleapis.com/support-kms-prod/SNP_67CE1E2EEBBDA24AC636DF84A911964629C8_3207874_en_v1" alt="Google Autocomplete" /></p>

<p>Adding such an option to your site or application might seem daunting but there is a very simple recursive data structure that solves the problem. There is a ton of literature on the net on how to do this using black box approaches like Lucene, Solr, Sphinx, Redis etc. But all these packages require a lot of configuration and you also lose flexibility. Tries can be implemented in a few lines of code in any language of your choice.</p>

<p><img align="middle" alt="Trie" height="280" src="http://upload.wikimedia.org/wikipedia/commons/thumb/b/be/Trie_example.svg/400px-Trie_example.svg.png" width="300" /></p>

<p>A trie is basically a tree, with each node representing a letter as illustrated in the figure above. Words are paths along this tree and the root node has no characters associated with it.</p>

<ol><li>The value of each node is the path or the character sequence leading upto it.</li>&#13;
<li>The children at each node are ideally represented using a hash table mapping the next character to the child nodes.</li>&#13;
<li>Its also useful to set a flag at every node to indicate whether a word/phrase ends there.</li>&#13;
</ol>
<p>Now we can define methods to insert a string and to search for one.</p>

<script src="https://gist.github.com/1906905.js?file=trie.py"></script>

<p>The insertion cost has a linear relationship with the string length. Now lets define the methods for listing all the strings which start with a certain prefix. The idea is to traverse down to the node representing the prefix and from there do a breadth first search of all the nodes that are descendants of the prefix node. We check if the node is at a word boundary from the flag we defined earlier and append the node’s value to the results. A caveat of the recursive approach, is you might run into stack depth problems when the maximum length of a string reaches tens of thousands of characters.</p>

<script src="https://gist.github.com/1906922.js?file=trie.py"></script>

<p>To delete an item,  first traverse down to the leaf of the keyword and then work backwards, checking for common paths.</p>

<p>And there you have it, an efficient way of retrieving strings with a common prefix in less than 40 lines of python code. I also wrote a C++ version using the STL data structures in about 90 lines, the time complexity for this version however is <strong>O(n log n)</strong> as the STL uses a red-black tree implementation for an associative array. Colin Dean has wrote a Ruby version and Marcus McCurdy, a Java one. You can find them all over here - <a href="https://github.com/vivekn/autocomplete">https://github.com/vivekn/autocomplete</a>. Read more about tries at <a href="http://en.wikipedia.org/wiki/Trie" title="Wikipedia" target="_blank">Wikipedia</a>.</p>

<p><strong>Update: </strong>Thanks to bmahler on reddit for pointing out that the wordlist approach was unnecessary and space inefficient. It has been corrected.</p>

<div style="display: none;">
	trie data structure <br />
	trie algorithm <br />
	trie autocomplete <br />
	autocomplete <br />
	autosuggest <br />
	autocomplete algorithm<br />
	autosuggest algorithm<br />
	trie implementation <br />
	input autocomplete <br />
	autocomplete algorithm <br />
</div>
]]></content>
  </entry>
  
</feed>
